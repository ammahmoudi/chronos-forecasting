context_length: 512
dataloader_num_workers: 1
gradient_accumulation_steps: 1
learning_rate: 0.001
log_steps: 200
lr_scheduler_type: linear
max_missing_prop: 0.9
max_steps: 1000
min_past: 60
model_id: amazon/chronos-t5-base
model_type: seq2seq
n_tokens: 4096
ntokens: 4096
num_samples: 20
optim: adamw_torch_fused
output_dir: ./experiment_configs_chronos_missing_analysis/seed_831363_model_amazon-chronos-t5-base_dtype_float32_mode_random_context_6_pred_9/patient_570/logs/logs_2025-03-17_22-55-57/chronos-t5-base
per_device_train_batch_size: 8
prediction_length: 64
probability:
- 1.0
random_init: false
save_steps: 500
seed: 831363
shuffle_buffer_length: 100000
tf32: true
tie_embeddings: true
tokenizer_class: MeanScaleUniformBins
tokenizer_kwargs: '{''low_limit'': -15,''high_limit'': 15}'
torch_compile: true
training_data_paths:
- ./data/missing_values/570-random-training.arrow
use_eos_token: true
warmup_ratio: 0.0
